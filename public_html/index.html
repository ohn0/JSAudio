<!DOCTYPE html>
<!--
To change this license header, choose License Headers in Project Properties.
To change this template file, choose Tools | Templates
and open the template in the editor.
-->
<html>
    <head>
        <title>Audio player tutorial</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel='stylesheet' type='text/css' href='tutorialCSS.css'>
    </head>
    <body>
        <h1 class="heading">Audio Player Tutorial</h1>
        <div id="overview">
        This is an audio player framework that lets the user insert music files
        and play them on their site. At the minimum, the user needs to supply a 
        div element and a mp3/ogg file into the framework to get the music player.
        The user can choose whether or not they want to include visualizations.
        <br/>
        I wanted to build an audio player and I managed to do that using the audio
        API provided. I also wanted to create some visualizations to go along with
        the music being played and was able to create three types of visuals: a bar,
        line, and circle visual. Creative names, right?<br/>
        This tutorial will allow the reader to get familiar with the audio API and
        learn how to extract data from the audio file and use it in canvas.
        </div>
        
        <h2 class="heading">Audio element</h2>
        <div class="info">
            <div class="audioText">If you want to play audio, you are going to need
            a basic audio player. Conveniently, we can create a basic player using
            the audio element as shown to the right. The only kinds of files the
            element can play are mp3, ogg, and wav. The codepen example won't let
            you play any of the files, though. Below is a working player with one file.
            <br/><br/>
            <div class='contents'>
            <audio controls='' src='music/YSO_014.ogg'></audio></div>
            </div>
            <div class="audioEle"><p data-height="265" data-theme-id="0" data-slug-hash="rYxNyg" data-default-tab="html" data-user="VVWV" data-embed-version="2" data-pen-title="rYxNyg" class="codepen">See the Pen <a href="https://codepen.io/VVWV/pen/rYxNyg/">rYxNyg</a> by ayyy (<a href="https://codepen.io/VVWV">@VVWV</a>) on <a href="https://codepen.io">CodePen</a>.</p>
            <script async src="https://production-assets.codepen.io/assets/embed/ei.js"></script></div>
        </div>
        <br/>
        <h2 class="heading">Audio API</h2>
        <div class="info">
            <div class="audioText">
                The audio element is pretty simple, but to build a visualizer, we
                need a way to access the data that the audio element is generating.
                This requires the usage of the audio API. The audio API gives the
                developer complete control over any audio content being played or
                generated, allowing the developer to route the data to intermediate nodes
                which can transform the audio data to add effects and modifications
                before the user can hear the sound.<br/><br/>
                <div class='contents'>
                <img alt src="https://mdn.mozillademos.org/files/12237/webaudioAPI_en.svg">
                </div>
                <br/> This picture shows how the audio API works on audio data.
                <ol>
                    <li>We first create an audio context object within which the
                        audio API will function.
                    <li>An input node is then defined, this can be an audio element 
                        playing a file, an audio source on the web, a microphone 
                        the user talks into, basically anything that generates noise.
                        This node outputs data converted from an audio signal which
                        can be used to modify the sound.
                    <li>The source output is then routed into 0 or more effects
                        nodes, these are functions that the user creates, or  
                        the API provides. These nodes take the input and transform
                        it in someway, like modifying the intensity, adding reverb,
                        or compressing the data.
                    <li>The last node is the destination node, this is where the 
                        sound will be played, like your speakers or headphones.
                </ol>
                The codepen to the right shows you how to create a basic audio context.
                Note that we just create a source and link it to a destination, we 
                are not processing the data in any way. Once again, codepen won't 
                let me use music files, so the working player is below. Note how
                if we don't connect the source to the destination, the audio
                file will play but you won't hear anything.<br/>
                <div class='contents'> 
                <audio id='step2' controls='' src='music/YSO_014.ogg'></audio>
                </div>
            </div>
            <div  class="audioEle"><p data-height="265" data-theme-id="0" data-slug-hash="rYxVgQ" data-default-tab="js,result" data-user="VVWV" data-embed-version="2" data-pen-title="rYxVgQ" class="codepen">See the Pen <a href="https://codepen.io/VVWV/pen/rYxVgQ/">rYxVgQ</a> by ayyy (<a href="https://codepen.io/VVWV">@VVWV</a>) on <a href="https://codepen.io">CodePen</a>.</p>
            <script async src="https://production-assets.codepen.io/assets/embed/ei.js"></script></div>
        </div>
        <br/>
        <h2 class='heading'>Extracting Data</h2>
        <div class='info'>
            <div class='audioText'>
                Now that you have some idea of how the audio API functions, we can
                add an intermediate node after the input node to read the data 
                the music file is currently generating.<br/>
                The audio file is being sampled at any given time and we can extract
                this data and put it into a buffer with the help of the audio API.
                We first need to set the size of the number of samples we want to 
                acquire. The higher this value, the more data we will have to work
                with, but this will also mean more work for the processor. 
                The number of samples also has to be a power of two. The
                default value for the tutorial is set to 512. However, the number
                of samples we will have to work with is NOT 512, it will be a half
                of the number of samples set, so we actually have 256 values to
                work with. Keep that in mind when you make your own later. <br/>
                The analyser node doesn't modify audio data, however it does 
                create a buffer that is filled with the sampled data as long as
                the song is playing. We can now access this buffer and extract
                the sampling data of the song while it is playing.<br/><br/>
                The snippet to the right shows how we create an intermediate node
                (analyser) and use it as a bridge between the input and output.
                Now whenever the audio file will be playing, the output will be
                first sent to the analyser node, and then forwarded to the 
                destination. We also create a Uint8Array, outputData, sending the
                size of analyser's bins to it. I am using a Uint8Array
                because this constrains all the sampled values between 0 and 255.
                which could be used to do something with RGB values.A function, updateVals, is
                created, which has the analyser node calling getByteFrequencyData.
                This function copies the current values of the analyser's 
                buffer into the buffer we provide(outputData). We then write
                those values to the screen. Finally we call setInterval and pass
                it updateVals with an interval of 50ms. Every 50ms, updateVals
                will be called, which will fill ouput data with the current
                sample values of the file.<br/>
                Play the audio player below and watch as all the values in the buffer 
                keep changing, these are the values the file is generating.<br/>
                <div class='contents'><audio id='step3' controls='' src='music/YSO_014.ogg'></audio>
                    <div id='audioData'></div>
                </div>
            </div>
            <div class='audioEle'><p data-height="265" data-theme-id="0" data-slug-hash="zPrKed" data-default-tab="js" data-user="VVWV" data-embed-version="2" data-pen-title="audio2" class="codepen">See the Pen <a href="https://codepen.io/VVWV/pen/zPrKed/">audio2</a> by ayyy (<a href="https://codepen.io/VVWV">@VVWV</a>) on <a href="https://codepen.io">CodePen</a>.</p>
            <script async src="https://production-assets.codepen.io/assets/embed/ei.js"></script></div>
        </div>
        <h2 class='heading'>Using the data with the canvas</h2>
        <div class='info'>
            <div class='audioText'>
                We are almost done, all we have to do now is associate the buffer
                with all the sampled values with some kind of visual. We'll do 
                a simple one, generating bars whose sizes change based on
                the values of the array.
                We first have to set up the canvas.<br/>
                Check the snippet's HTML tab to see how to create a canvas element.
                Once we have the element, check the JS tab to see the script we are using.
                We get the canvas element and get a context object(cCtx). We then
                have a function, visual, which will be drawing the bars in the canvas.
                The first thing we do is we clear the canvas(clearRect), we then 
                define rectWidth as the length of a single bar, which will be the
                total width of the canvas divided by the number of bars we will be
                drawing. We then enter a loop where we constantly call fillRect, which
                draws a rectangle at the location specified by the first two parameters,
                and uses the last 2 parameters as the dimension of the rectangle, width
                and height. 
                Once we escape the loop we call requestAnimationFrame, which is like
                setInterval, but is more efficient for canvas objects, as it doesn't
                update the screen if we aren't focused on the window. Try to 
                understand how the fillRect method is drawing each rect, I think 
                it will help you if you want to make your own visualizations.
                Use the audio player from the top to see the visualizations.<br/>
                <div class='contents'>
                <canvas id='canvasA' width='400' height='400'></canvas>
                </div>
            </div>
            <div class='audioEle'><p data-height="265" data-theme-id="0" data-slug-hash="JOGWry" data-default-tab="js" data-user="VVWV" data-embed-version="2" data-pen-title="audio4" class="codepen">See the Pen <a href="https://codepen.io/VVWV/pen/JOGWry/">audio4</a> by ayyy (<a href="https://codepen.io/VVWV">@VVWV</a>) on <a href="https://codepen.io">CodePen</a>.</p>
<script async src="https://production-assets.codepen.io/assets/embed/ei.js"></script></div>
        </div>
        <script src='tutorialScripts.js'></script>
        <h2 class='heading'>Summary</h2>
        <div id='summary'>If you've made it this far, that probably means you 
        tried the examples and attempted to build your own visualizer. I hope
        that went well for you. My experience with it was pretty rough when I began
        working on the visualizer. Creating the element was pretty easy, the 
        tough part was figuring out how to use the audio context to get the 
        sampling data from the music file. I spent a good 4 hours looking over 
        20 lines of javascript trying to figure out why the data wasn't being
        transferred properly. After consulting the documentation multiple times and
        just trying any combination I could think of, the audio context finally
        decided to work properly. The funny part is I still don't know what I fixed 
        to read the data into my buffer. But I got it to work at the end and after that all I had
        to do was link the data into the canvas element. That was a pretty interesting
        part as I had to figure out just how I would draw each bar into the canvas
        to show the visual, it was confusing at first because I forgot that the
        Y-axis starts at 0 on the top left and increases as you go DOWN the screen.
        Once I got that mistake taken care of, the rest of the visualizations were
        just messing with some math and drawing it into the screen, not so bad.
        </div>
        
    </body>
</html>
